---
format:
  pdf:
    documentclass: ../MastersDoctoralThesis
    classoption: ["11pt", "english", "singlespacing", "headsepline"]
---

# Data Exploration and Validation {#sec-Chapter1}

## Dataset Overview

This chapter details the initial data exploration and validation process for the dune vegetation analysis project. The dataset consists of a primary Excel workbook (`db_species_20250214.xlsx`) containing several sheets with information about:

1. Species presence/abundance at different sampling plots (`original_data` sheet)
2. Land cover percentages at 50m and 100m radii from sample points (province-specific sheets)
3. Management practices and protection statuses for beach areas (province-specific sheets)

The data is structured by geographic region (Girona, Barcelona, and Tarragona provinces) and contains information about different beaches, transects, and plots within each region.

## Data Processing Steps

The data processing workflow follows these key steps:

1. Loading and cleaning the main dataset
2. Processing land cover data for each province 
3. Processing management data for each province
4. Validating the processed data files

All processing was implemented in R using the tidyverse ecosystem of packages, with special emphasis on data cleaning and transformation.

### Loading the Main Dataset

The main dataset containing species observations was loaded from the `original_data` sheet of the Excel workbook. This process involved:

```{r}
#| label: load-packages
#| echo: true
#| eval: false
# Load required packages
library(tidyverse)
library(readxl)
library(conflicted)
library(ggplot2)
library(janitor) # For clean_names function
library(dplyr)
```

```{r}
#| label: load-data
#| echo: true
#| eval: false
# Load the main data sheet
main_data <- read_excel("data/db_species_20250214.xlsx", sheet = "original_data")
main_data <- main_data %>% select(where(~ !all(is.na(.))))

# Clean column names using the janitor package
main_data <- main_data %>% janitor::clean_names()
```

This initial loading process removed completely empty columns and standardized column names using the `clean_names()` function, which converts column names to lowercase, replaces spaces with underscores, and removes special characters.

### Numeric Column Conversion

The species abundance data needed to be converted to numeric format for analysis. The script identified the relevant columns and performed the conversion:

```{r}
#| label: numeric-conversion
#| echo: true
#| eval: false
# Find the index of the EUNIS column
eunis_col_index <- which(grepl("eunis", names(main_data), ignore.case = TRUE))
if(length(eunis_col_index) == 0) {
  cat("Warning: Could not find 'EUNIS' column. Will parse all columns from second onwards.\n")
  eunis_col_index <- ncol(main_data) + 1  # Set to beyond the last column
} else {
  eunis_col_index <- min(eunis_col_index)  # Take the first match if multiple
}

# Parse columns from second to EUNIS as numeric
for(i in 2:min(ncol(main_data), eunis_col_index - 1)) {
  col_name <- names(main_data)[i]
  # Store the original values to check for parsing issues
  original_values <- main_data[[i]]
  
  # Try to convert to numeric
  main_data[[i]] <- as.numeric(as.character(main_data[[i]]))
  
  # Check if we lost any non-NA values
  if(sum(!is.na(original_values)) > sum(!is.na(main_data[[i]]))) {
    warning_msg <- paste("Warning: Some values in column", col_name, "could not be parsed as numeric")
    cat(warning_msg, "\n")
    
    # Report the problematic values
    problematic <- original_values[!is.na(original_values) & is.na(main_data[[i]])]
    if(length(problematic) > 0) {
      cat("  Problematic values:", toString(head(unique(problematic), 5)), "\n")
    }
  }
}
```

This conversion process included validation checks to identify any values that could not be properly converted to numeric format, ensuring data quality through the transformation.

### Column Reordering

To ensure consistency in data structure, the columns were reordered to follow a specific pattern:

```{r}
#| label: column-reorder
#| echo: true
#| eval: false
# Identify all desired columns
species_cols <- setdiff(names(main_data),
                      c("plot", "id_beach", "beach", "id_transect", "id_plot", "transect", "eunis"))

# Create the desired column order
ordered_cols <- c("plot", "id_beach", "beach", "id_transect", "id_plot", "transect", "eunis", species_cols)
# Reorder columns (only those that exist)
main_data <- main_data %>% select(all_of(ordered_cols), everything())

# Save the processed main data
save(main_data, file = "data/processed_data_clean.RData")
```

The reordering placed identifier columns first, followed by the species columns, making the data structure more intuitive for subsequent analysis.

### Processing Land Cover Data

For each province (Girona, Barcelona, and Tarragona), the land cover data was processed using a dedicated function:

```{r}
#| label: land-cover-processing
#| echo: true
#| eval: false
# Function to process land cover sheets
process_land_cover <- function(sheet_name) {
  cat("Processing sheet:", sheet_name, "\n")
  
  # Read the sheet
  land_cover_data <- read_excel("data/db_species_20250214.xlsx", sheet = sheet_name)
  
  # Clean column names
  land_cover_data <- land_cover_data %>% janitor::clean_names()
  
  # Select id_beach/id_plot column
  id_col <- grep("^id_beach$|^id_plot$", names(land_cover_data), value = TRUE)[1]
  if(is.na(id_col)) {
    id_col <- grep("id.*beach|beach.*id|id.*plot|plot.*id", names(land_cover_data), value = TRUE)[1]
  }
  
  # Get 50m and 100m columns
  cols_50m <- grep("^(x)?50m_", names(land_cover_data), value = TRUE)
  cols_100m <- grep("^(x)?100m_", names(land_cover_data), value = TRUE)
  
  if(length(cols_50m) == 0) {
    cols_50m <- grep("50.*m|50m|50 m", names(land_cover_data), value = TRUE)
  }
  
  if(length(cols_100m) == 0) {
    cols_100m <- grep("100.*m|100m|100 m", names(land_cover_data), value = TRUE)
  }
  
  # Select columns
  distance_cols <- c(cols_50m, cols_100m)
  selected_cols <- c(id_col, distance_cols)
  
  # Filter data
  filtered_data <- land_cover_data %>% select(all_of(selected_cols)) %>% distinct()
  
  # Convert columns to numeric if needed
  for(col in distance_cols) {
    filtered_data[[col]] <- as.numeric(filtered_data[[col]])
  }
  
  # Convert ID column to integer
  filtered_data[[id_col]] <- as.integer(filtered_data[[id_col]])
  
  return(filtered_data)
}

# Process each land cover sheet
girona_land_cover <- process_land_cover("girona_land cover")
barcelona_land_cover <- process_land_cover("barcelona_land cover")
tarragona_land_cover <- process_land_cover("tarragona_land cover")

# Create a list containing all land cover datasets
land_cover_data <- list(
  "Girona" = girona_land_cover,
  "Barcelona" = barcelona_land_cover,
  "Tarragona" = tarragona_land_cover
)

# Save the combined land cover data
save(land_cover_data, file = "data/all_land_cover_data.RData")
```

This processing function:

1. Identifies the ID column (beach or plot identifier)
2. Extracts columns related to 50m and 100m land cover measurements
3. Filters to keep only necessary columns
4. Converts all measurements to numeric format
5. Returns a standardized data frame for each province

### Processing Management Data

The management data followed a similar processing pattern, but with special handling for categorical variables:

```{r}
#| label: management-data-processing
#| echo: true
#| eval: false
# Function to process management sheets
process_management <- function(sheet_name) {
  cat("Processing sheet:", sheet_name, "\n")
  
  # Read the sheet
  management_data <- read_excel("data/db_species_20250214.xlsx", sheet = sheet_name)
  
  # Clean column names
  management_data <- management_data %>% janitor::clean_names()
  
  # Standardize key column names based on what is expected
  expected_cols <- c(
    "id_plot", "id_beach", "beach", 
    "managed_paths", "rope_fences", "mechanical_cleaning",
    "surface_area_occupied_by_seasonal_services_and_amenities_on_or_less_than_5_m_from_the_dunes",
    "surface_area_of_parking_or_other_fixed_services_on_or_less_than_5_m_from_the_dunes",
    "protection_of_the_system_and_the_immediate_environment",
    "degree_of_protection_according_to_the_iucn_classification"
  )
  
  # Find matching columns
  actual_cols <- vector("character", length(expected_cols))
  for (i in seq_along(expected_cols)) {
    pattern <- expected_cols[i]
    simple_pattern <- gsub("_", ".*", pattern)
    matches <- grep(simple_pattern, names(management_data), ignore.case = TRUE, value = TRUE)
    
    if (length(matches) > 0) {
      actual_cols[i] <- matches[1]
    } else {
      cat("Warning: Could not find a column matching '", expected_cols[i], "'\n", sep = "")
      actual_cols[i] <- NA
    }
  }
  
  # Remove NA values
  actual_cols <- actual_cols[!is.na(actual_cols)]
  
  if (length(actual_cols) > 0) {
    # Subset the original data
    filtered_data <- management_data %>% select(all_of(actual_cols))
    
    # Detect ID columns
    id_plot_col <- grep("id.*plot|plot.*id", names(filtered_data), ignore.case = TRUE, value = TRUE)[1]
    id_beach_col <- grep("id.*beach|beach.*id", names(filtered_data), ignore.case = TRUE, value = TRUE)[1]
    
    # Ensure ID columns are integers
    if (!is.na(id_plot_col)) {
      filtered_data[[id_plot_col]] <- as.integer(filtered_data[[id_plot_col]])
    }
    
    if (!is.na(id_beach_col)) {
      filtered_data[[id_beach_col]] <- as.integer(filtered_data[[id_beach_col]])
    }
    
    # Convert appropriate columns to factors if they have categorical values
    for (col in names(filtered_data)) {
      if (is.character(filtered_data[[col]]) && 
          !grepl("^id|^beach$", col, ignore.case = TRUE)) {
        unique_vals <- unique(na.omit(filtered_data[[col]]))
        if (length(unique_vals) < 10) {  # Assume categorical if fewer than 10 unique values
          filtered_data[[col]] <- factor(filtered_data[[col]])
        }
      }
    }
    
    return(filtered_data)
  } else {
    warning("No usable columns found in the management sheet")
    return(NULL)
  }
}

# Process each management sheet
girona_management <- process_management("girona_management")
barcelona_management <- process_management("barcelona_management")
tarragona_management <- process_management("tarragona_management")

# Create a list containing all management datasets
management_data <- list(
  "Girona" = girona_management,
  "Barcelona" = barcelona_management,
  "Tarragona" = tarragona_management
)

# Save the combined management data
save(management_data, file = "data/all_management_data.RData")
```

This processing function:

1. Attempts to locate columns matching expected management variables
2. Converts ID columns to integer format
3. Intelligently converts categorical variables to factors
4. Returns a standardized data frame for each province

## Data Validation

After the initial data processing, a validation script was executed to ensure data quality and integrity. The validation process included:

1. Checking the dimensions of the processed datasets
2. Verifying column presence and order
3. Validating value ranges and formats
4. Cross-checking relationships between identifier fields

### Main Data Validation

The main data validation included these key checks:

```{r}
#| label: main-data-validation
#| echo: true
#| eval: false
validate_main_data <- function(data) {
  cat("===== Validating main_data =====\n\n")
  
  # Check dimensions
  expected_rows <- 278
  expected_cols <- 147 + 7  # Species columns + identifier columns
  
  # Check column existence and order
  expected_first_cols <- c("plot", "id_beach", "beach", "id_transect", "id_plot", "transect", "eunis")
  
  # Validate column formats
  # Check plot format (D+_D+_D+)
  if ("plot" %in% names(data)) {
    plot_format_check <- all(grepl("^\\d+_\\d+_\\d+$", data$plot))
  }
  
  # Validate that plot is the concatenation of id_beach, id_transect, and id_plot
  if (all(c("plot", "id_beach", "id_transect", "id_plot") %in% names(data))) {
    # Build expected plot values
    expected_plot <- paste(data$id_beach, data$id_transect, data$id_plot, sep = "_")
    plot_match <- all(data$plot == expected_plot)
  }
  
  # Validate all species columns have values between 0 and 5
  first_species_idx <- max(which(names(data) %in% expected_first_cols)) + 1
  
  if (first_species_idx <= ncol(data)) {
    species_cols <- names(data)[first_species_idx:ncol(data)]
    
    # Function to check if column has valid values (0-5 or NA)
    check_species_col <- function(col_name) {
      values <- data[[col_name]]
      valid_values <- is.na(values) | (values >= 0 & values <= 5 & values == floor(values))
      return(all(valid_values))
    }
    
    # Apply check to all species columns
    species_check_results <- sapply(species_cols, check_species_col)
  }
}
```

### Land Cover Data Validation

Validation for the land cover data focused on:

```{r}
#| label: land-cover-validation
#| echo: true
#| eval: false
validate_land_cover_data <- function(data) {
  # Check if it's a list with expected regions
  expected_regions <- c("Girona", "Barcelona", "Tarragona")
  expected_rows <- c("Girona" = 19, "Barcelona" = 4, "Tarragona" = 16)
  
  # Expected column patterns for land cover data
  id_col_pattern <- "^id_beach$"
  expected_50m_patterns <- c(
    "scrubland", "grassland", "communication_routes", "urban", 
    "forestry_bare_soil", "forests", "lagoon_and_salt_marshes", 
    "crops", "freshwater"
  )
  
  # For each region, check:
  # - Number of columns (expect 19: id_beach + 9 for 50m + 9 for 100m)
  # - Presence of expected land cover types
  # - That all columns are numeric
  # - That 50m and 100m percentages sum to 100% for each row
}
```

The key validation for the land cover data was ensuring that the percentages for different land cover types at each distance (50m and 100m) summed to approximately 100%.

### Management Data Validation

The management data validation focused on:

```{r}
#| label: management-validation
#| echo: true
#| eval: false
validate_management_data <- function(data) {
  # Check if it's a list with expected regions
  expected_regions <- c("Girona", "Barcelona", "Tarragona")
  expected_rows <- c("Girona" = 19, "Barcelona" = 4, "Tarragona" = 16)
  
  # Expected columns for management data
  expected_columns <- c("id_plot", "id_beach", 
                      "managed_paths", "rope_fences", "mechanical_cleaning",
                      "surface_area_occupied_by_seasonal_services_and_amenities_on_or_less_than_5_m_from_the_dunes",
                      "surface_area_of_parking_or_other_fixed_services_on_or_less_than_5_m_from_the_dunes",
                      "protection_of_the_system_and_the_immediate_environment",
                      "degree_of_protection_according_to_the_iucn_classification")
  
  # For each region, check:
  # - Presence of expected columns
  # - That management rating columns only contain integer values 0-5
}
```

The most important validation for management data was ensuring that the management rating columns contained only integer values between 0 and 5.

## Summary

The data exploration and validation process established a solid foundation for further analysis by:

1. Loading and cleaning the raw data from the Excel workbook
2. Standardizing column names and formats
3. Converting data types appropriately for future analysis
4. Structuring the data consistently across geographical regions
5. Validating the integrity and quality of the processed datasets

The resulting processed datasets were saved as RData files for efficient loading in subsequent analysis steps:

1. `processed_data_clean.RData` - Main species observations dataset
2. `all_land_cover_data.RData` - Land cover percentages for each province
3. `all_management_data.RData` - Management practices for each province

These processed files form the basis for the exploratory data analysis and modeling presented in subsequent chapters.
